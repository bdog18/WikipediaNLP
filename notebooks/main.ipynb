{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e677e61",
   "metadata": {},
   "source": [
    "All of this is ran in a docker container using the following image:\n",
    "\n",
    "nvcr.io/nvidia/tensorflow:23.12-tf2-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ce7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add root directory (one level up from notebooks/)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc0ea6",
   "metadata": {},
   "source": [
    "download and install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de0b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikiextractor already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(r\"../wikiextractor-master\"):\n",
    "    # Step 1: Download the ZIP file\n",
    "    !curl -L -o ../wikiextractor.zip https://github.com/qfcy/wikiextractor/archive/refs/heads/master.zip\n",
    "\n",
    "    # Step 2: Extract it\n",
    "    import zipfile\n",
    "    import os\n",
    "\n",
    "    zip_path = r\"../wikiextractor.zip\"\n",
    "    extract_to = r\"../\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Step 3: Delete the ZIP file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    # Step 4: Install Wikiextractor\n",
    "    !pip install -e ../wikiextractor-master\n",
    "else:\n",
    "    print(\"Wikiextractor already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990a714",
   "metadata": {},
   "source": [
    "Get wikipedia dump (takes like 2 hours to download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50a29ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dump already downloaded\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "if not os.path.isfile(r\"../data/raw/enwiki-latest-pages-articles.xml.bz2\"):\n",
    "    !wget -P ../data/raw https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
    "else:\n",
    "    print(\"Wikipedia dump already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a9caa",
   "metadata": {},
   "source": [
    "Extract xml data from wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b1a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia XML extract already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(r\"../data/raw/extracted_wikidata\"):\n",
    "    !python -m wikiextractor.WikiExtractor \\\n",
    "        ../data/raw/enwiki-latest-pages-articles.xml.bz2 \\\n",
    "        -o ../data/raw/extracted_wikidata \\\n",
    "        --no-templates\n",
    "else:\n",
    "    print(\"Wikipedia XML extract already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0923b64",
   "metadata": {},
   "source": [
    "Create json data from wiki-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6511a9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikidata_json already exists\n"
     ]
    }
   ],
   "source": [
    "from utils.data_prep import traverse_directory\n",
    "\n",
    "input_dir = r'../data/raw/extracted_wikidata'\n",
    "output_dir = r'../data/processed/wikidata_json'\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    traverse_directory(input_dir, output_dir)\n",
    "else:\n",
    "    print(\"wikidata_json already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9736f8",
   "metadata": {},
   "source": [
    "Initialize spark session for metadata and create random triplets\n",
    "\n",
    "Create metadata from articles\n",
    "\n",
    "Create Training data of random triplets using Pyspark from json wikidata\n",
    "\n",
    "Had to install winutils for hadoop and pyspark to work on windows locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e93932",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"../data/processed/wikidata_json\"\n",
    "output_dir = r\"../data/processed/triplets/parts\"\n",
    "metadata_path = r\"../data/custom_model/article_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 19:55:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/14 19:55:29 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplets data already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines across training files: 4091164\n"
     ]
    }
   ],
   "source": [
    "from utils.spark_functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Capstone\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"20g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.local.dir\", \"../spark-temp\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "if not os.path.isdir(output_dir) or not os.path.isfile(metadata_path):\n",
    "    print(\"Loading JSON input\")\n",
    "    json_df = spark.read.option(\"multiLine\", True).json(f\"{input_dir}/**/*.json\")\n",
    "\n",
    "    # Create metadata file from articles\n",
    "    create_article_metadata(json_df, metadata_path)\n",
    "\n",
    "    # Logic for creating training data triplets from json\n",
    "    df = create_paragraphs_df(json_df)\n",
    "    triplets = create_random_triplets(df)\n",
    "    print(f\"Writing triplets to Spark part files: {output_dir}\")\n",
    "    triplets.write.mode(\"overwrite\").json(output_dir)\n",
    "\n",
    "else:\n",
    "    print(\"Triplets data already exists\")\n",
    "\n",
    "# Calculate total lines so that we can determine epoch size\n",
    "total_lines = spark.read.json(f\"{input_dir}/*.json\", multiLine=False).count()\n",
    "print(\"Total lines across training files:\", total_lines)\n",
    "\n",
    "# Stop spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf9cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_prep import delete_files_in_dir_based_on_ext\n",
    "\n",
    "# Delete unneeded files produced by pyspark\n",
    "delete_files_in_dir_based_on_ext(output_dir, \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842330",
   "metadata": {},
   "source": [
    "Training embedding model using Random Triplets Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b53936",
   "metadata": {},
   "source": [
    "grid search on smaller subset of data for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee19087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 04:08:46.117175: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 04:08:46.431699: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-16 04:08:46.431760: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-16 04:08:46.431942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 04:08:46.570366: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "from utils.custom_embedder import *\n",
    "\n",
    "# Turn off warnings for Tensorflow \n",
    "tf_logger = logging.getLogger(\"tensorflow\")\n",
    "tf_logger.setLevel(logging.ERROR)\n",
    "\n",
    "# So we don't have to rerun it every time\n",
    "total_lines = 4091164 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcea831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved vectorizer\n",
      "ðŸ”§ Training config 0: {'embed_dim': 128, 'num_heads': 4, 'ff_dim': 256, 'num_layers': 2, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 182s 216ms/step - loss: 0.2919\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 172s 215ms/step - loss: 0.2663\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 170s 213ms/step - loss: 0.2224\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 170s 212ms/step - loss: 0.1761\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 168s 211ms/step - loss: 0.1381\n",
      "ðŸ”§ Training config 1: {'embed_dim': 128, 'num_heads': 4, 'ff_dim': 256, 'num_layers': 3, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 183s 217ms/step - loss: 0.2937\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 173s 217ms/step - loss: 0.2730\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 172s 215ms/step - loss: 0.2382\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 171s 214ms/step - loss: 0.1963\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 170s 213ms/step - loss: 0.1619\n",
      "ðŸ”§ Training config 2: {'embed_dim': 128, 'num_heads': 4, 'ff_dim': 512, 'num_layers': 2, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 178s 212ms/step - loss: 0.2919\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 175s 220ms/step - loss: 0.2663\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 183s 229ms/step - loss: 0.2233\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 195s 245ms/step - loss: 0.1781\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 174s 218ms/step - loss: 0.1430\n",
      "ðŸ”§ Training config 3: {'embed_dim': 128, 'num_heads': 4, 'ff_dim': 512, 'num_layers': 3, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 193s 228ms/step - loss: 0.2941\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 180s 225ms/step - loss: 0.2736\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 175s 220ms/step - loss: 0.2421\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 174s 218ms/step - loss: 0.2045\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 172s 216ms/step - loss: 0.1717\n",
      "ðŸ”§ Training config 4: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 256, 'num_layers': 2, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 180s 214ms/step - loss: 0.2931\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 171s 214ms/step - loss: 0.2725\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 170s 213ms/step - loss: 0.2397\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 173s 216ms/step - loss: 0.2022\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 174s 218ms/step - loss: 0.1706\n",
      "ðŸ”§ Training config 5: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 256, 'num_layers': 3, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 189s 223ms/step - loss: 0.2968\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 177s 222ms/step - loss: 0.2807\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 175s 219ms/step - loss: 0.2529\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 173s 217ms/step - loss: 0.2192\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 176s 220ms/step - loss: 0.1899\n",
      "ðŸ”§ Training config 6: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 512, 'num_layers': 2, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 188s 224ms/step - loss: 0.2949\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 184s 231ms/step - loss: 0.2743\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 176s 221ms/step - loss: 0.2423\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 178s 222ms/step - loss: 0.2041\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 172s 215ms/step - loss: 0.1727\n",
      "ðŸ”§ Training config 7: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 512, 'num_layers': 3, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 191s 226ms/step - loss: 0.2948\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 183s 229ms/step - loss: 0.2769\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 178s 223ms/step - loss: 0.2489\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 178s 222ms/step - loss: 0.2160\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 175s 219ms/step - loss: 0.1899\n",
      "ðŸ”§ Training config 8: {'embed_dim': 256, 'num_heads': 4, 'ff_dim': 256, 'num_layers': 2, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "799/799 [==============================] - 187s 220ms/step - loss: 0.3011\n",
      "Epoch 2/5\n",
      "799/799 [==============================] - 173s 216ms/step - loss: 0.2862\n",
      "Epoch 3/5\n",
      "799/799 [==============================] - 170s 213ms/step - loss: 0.2612\n",
      "Epoch 4/5\n",
      "799/799 [==============================] - 169s 212ms/step - loss: 0.2212\n",
      "Epoch 5/5\n",
      "799/799 [==============================] - 170s 213ms/step - loss: 0.1788\n",
      "ðŸ”§ Training config 9: {'embed_dim': 256, 'num_heads': 4, 'ff_dim': 256, 'num_layers': 3, 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node custom_encoder_18/transformer_block_61/layer_normalization_123/batchnorm_2/mul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 518, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/tmp/ipykernel_88948/72152965.py\", line 42, in <module>\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 203, in train_with_config\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 79, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 46, in call\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 47, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 32, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/layer_normalization.py\", line 297, in call\n\nfailed to allocate memory\n\t [[{{node custom_encoder_18/transformer_block_61/layer_normalization_123/batchnorm_2/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_398073]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(configs):\n\u001b[0;32m---> 42\u001b[0m     loss, cfg, ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4091164\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((loss, cfg, ckpt))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Sort by loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py:203\u001b[0m, in \u001b[0;36mtrain_with_config\u001b[0;34m(config, vectorizer, input_dir, batch_size, steps_per_epoch, run_id)\u001b[0m\n\u001b[1;32m    197\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    198\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    199\u001b[0m     ModelCheckpoint(checkpoint_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m ]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_loss, config, checkpoint_path\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node custom_encoder_18/transformer_block_61/layer_normalization_123/batchnorm_2/mul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 529, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 518, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 424, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 766, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/tmp/ipykernel_88948/72152965.py\", line 42, in <module>\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 203, in train_with_config\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 79, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 46, in call\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 47, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/opt/files/Capstone/WikipediaNLP/utils/custom_embedder.py\", line 32, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/layer_normalization.py\", line 297, in call\n\nfailed to allocate memory\n\t [[{{node custom_encoder_18/transformer_block_61/layer_normalization_123/batchnorm_2/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_398073]"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_dir = \"../data/processed/triplets/parts_test\"\n",
    "vectorizer_dir = \"../data/custom_model/saved_vectorizer\"\n",
    "weights_dir = \"../data/custom_model/encoder_weights\"\n",
    "vocab_size = 30000\n",
    "max_len = 32\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "batch_size = 512\n",
    "num_epochs = 30\n",
    "\n",
    "# Load or create vectorizer\n",
    "if os.path.exists(vectorizer_dir):\n",
    "    print(\"Loading saved vectorizer\")\n",
    "    vectorizer = tf.keras.models.load_model(vectorizer_dir)\n",
    "else:\n",
    "    vectorizer = create_vectorizer(input_dir, vectorizer_dir, vocab_size, max_len)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"embed_dim\": [128, 256],\n",
    "    \"num_heads\": [4, 8],\n",
    "    \"ff_dim\": [256, 512],\n",
    "    \"num_layers\": [2, 3],\n",
    "    \"learning_rate\": [5e-4],\n",
    "}\n",
    "\n",
    "keys, values = zip(*search_space.items())\n",
    "configs = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "results = []\n",
    "for i, config in enumerate(configs):\n",
    "    loss, cfg, ckpt = train_with_config(config, vectorizer, input_dir, batch_size, (total_lines // batch_size)//10, run_id=i)\n",
    "    results.append((loss, cfg, ckpt))\n",
    "\n",
    "# Sort by loss\n",
    "results.sort(key=lambda x: x[0])\n",
    "\n",
    "# Print best\n",
    "best_loss, best_config, best_checkpoint = results[0]\n",
    "print(\"ðŸ† Best Config:\", best_config)\n",
    "print(\"ðŸ“‰ Best Loss:\", best_loss)\n",
    "print(\"ðŸ’¾ Best Checkpoint:\", best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4bd06",
   "metadata": {},
   "source": [
    "Embed and Index Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13987c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved vectorizer\n",
      "Epoch 1/30\n",
      "1500/7990 [====>.........................] - ETA: 29:31 - loss: 0.2886"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m\n\u001b[1;32m     30\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     32\u001b[0m     ModelCheckpoint(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# infinite generator\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_lines\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Save final encoder weights\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# encoder.save_weights(f\"{weights_dir}/final_encoder.weights.h5\")\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete and weights saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_dir = \"../data/processed/triplets/parts\"\n",
    "vectorizer_dir = \"../data/custom_model/saved_vectorizer\"\n",
    "weights_dir = \"../data/custom_model/encoder_weights\"\n",
    "vocab_size = 30000\n",
    "max_len = 32\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "batch_size = 512\n",
    "num_epochs = 30\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Load or create vectorizer\n",
    "if os.path.exists(vectorizer_dir):\n",
    "    print(\"Loading saved vectorizer\")\n",
    "    vectorizer = tf.keras.models.load_model(vectorizer_dir)\n",
    "else:\n",
    "    vectorizer = create_vectorizer(input_dir, vectorizer_dir, vocab_size, max_len)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = load_triplet_dataset_streamed(input_dir, vectorizer, batch_size)\n",
    "\n",
    "# Model setup\n",
    "encoder = CustomEncoder(vocab_size, max_len, embed_dim, num_heads, ff_dim)\n",
    "trainer = TripletTrainer(encoder)\n",
    "trainer.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"loss\", patience=2),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{weights_dir}/best_encoder.weights.h5\",\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training\n",
    "trainer.fit(\n",
    "    train_dataset.repeat(),  # infinite generator\n",
    "    steps_per_epoch=total_lines // batch_size,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"Training complete and weights saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved embeddings to 'wiki_anchor_embeddings.npy'\n"
     ]
    }
   ],
   "source": [
    "vectorizer = tf.keras.models.load_model(vectorizer_dir)\n",
    "\n",
    "triplet_dataset = load_triplet_dataset_streamed(input_dir, vectorizer, batch_size)\n",
    "\n",
    "anchor_embeddings = []\n",
    "\n",
    "for anchor_batch, _, _ in triplet_dataset:\n",
    "    emb = encoder(anchor_batch, training=False)  # shape (batch_size, embed_dim)\n",
    "    anchor_embeddings.append(emb.numpy())\n",
    "\n",
    "all_anchor_embeddings = np.concatenate(anchor_embeddings, axis=0)\n",
    "np.save(\"../data/custom_model/embeddings_output/wiki_anchor_embeddings.npy\", all_anchor_embeddings)\n",
    "print(\"âœ… Saved embeddings to 'wiki_anchor_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb2e0e",
   "metadata": {},
   "source": [
    "Create faiss index if not already existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dacdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 embedding files. Building FAISS index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fade041b3c4521a10f79f6109bbae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing embedding files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "from utils.faiss_index import create_faiss_index_from_dir\n",
    "\n",
    "embedding_path = \"../data/custom_model/embeddings_output\"\n",
    "faiss_path = \"../data/custom_model/faiss/faiss_index.index\"\n",
    "\n",
    "# Creates embeddings and faiss.index\n",
    "create_faiss_index_from_dir(embedding_path, faiss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdaaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../data/processed/triplets/parts\"\n",
    "vectorizer_dir = \"../data/custom_model/saved_vectorizer\"\n",
    "weights_dir = \"../data/custom_model/encoder_weights\"\n",
    "vocab_size = 30000\n",
    "max_len = 32\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Load saved vectorizer\n",
    "vectorizer = tf.keras.models.load_model(vectorizer_dir)\n",
    "vectorizer = vectorizer.layers[0]\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = CustomEncoder(vocab_size, max_len, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Build the model by calling it on dummy data\n",
    "# _ = encoder(tf.constant([[1] * 32]))  # shape: (1, max_len)\n",
    "\n",
    "# Now load the weights\n",
    "encoder.load_weights(f\"{weights_dir}/best_encoder.weights.h5\")\n",
    "\n",
    "# Your query\n",
    "query = \"What is the function of DNA?\"\n",
    "\n",
    "query_seq = vectorizer(tf.constant([query]))\n",
    "query_embedding = encoder(query_seq).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e839dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.faiss_index import query_faiss\n",
    "faiss_path = \"../data/custom_model/faiss/faiss_index.index\"\n",
    "indices = query_faiss(faiss_path, query_embedding, 10)\n",
    "\n",
    "# Load article metadata\n",
    "metadata_path = \"../data/custom_model/article_metadata.json\"\n",
    "with open(metadata_path, encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Retrieve top-k articles\n",
    "results = [metadata[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a01108fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': \"Yangon Children's Hospital\",\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=22418344'},\n",
      " {'title': 'Twitterature',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=43700363'},\n",
      " {'title': 'Peddapuram Assembly constituency',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=37904163'},\n",
      " {'title': 'Mount Buller (Victoria)',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=6211759'},\n",
      " {'title': 'Villanova Wildcats football',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=15373239'}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedding_path = \"../data/custom_model/embeddings_output/wiki_anchor_embeddings.npy\"\n",
    "\n",
    "# Normalize for cosine similarity\n",
    "query_vec = query_embedding[0].reshape(1, -1)\n",
    "article_embeddings = np.load(embedding_path)\n",
    "top_articles = [metadata[i] | {\"vec\": article_embeddings[i]} for i in indices[0]]\n",
    "\n",
    "# Rerank\n",
    "top_articles.sort(\n",
    "    key=lambda x: cosine_similarity(query_vec, x[\"vec\"].reshape(1, -1))[0][0],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "final_results = [\n",
    "    {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": article.get(\"url\", \"N/A\")\n",
    "    }\n",
    "    for article in top_articles[:5]\n",
    "]\n",
    "\n",
    "# Print nicely\n",
    "import pprint\n",
    "pprint.pprint(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Top-1 Accuracy: 0.0\n",
      "Top-3 Accuracy: 0.0\n",
      "Top-5 Accuracy: 0.0\n",
      "Top-10 Accuracy: 0.0\n",
      "Precision@1: 0.0\n",
      "Precision@3: 0.0\n",
      "Precision@5: 0.0\n",
      "Precision@10: 0.0\n",
      "Recall@1: 0.0\n",
      "Recall@3: 0.0\n",
      "Recall@5: 0.0\n",
      "Recall@10: 0.0\n",
      "MRR: 0.0\n"
     ]
    }
   ],
   "source": [
    "from utils.top_k_testing import evaluate_all_metrics, retrieval_function, load_test_set\n",
    "\n",
    "test_set = load_test_set(\"../data/test_data/test_queries.json\")\n",
    "results = evaluate_all_metrics(test_set, retrieval_function)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
