{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e677e61",
   "metadata": {},
   "source": [
    "All of this is ran in a docker container using the following image:\n",
    "\n",
    "nvcr.io/nvidia/tensorflow:23.12-tf2-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ce7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add root directory (one level up from notebooks/)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc0ea6",
   "metadata": {},
   "source": [
    "download and install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikiextractor already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(r\"../wikiextractor-master\"):\n",
    "    # Step 1: Download the ZIP file\n",
    "    !curl -L -o ../wikiextractor.zip https://github.com/qfcy/wikiextractor/archive/refs/heads/master.zip\n",
    "\n",
    "    # Step 2: Extract it\n",
    "    import zipfile\n",
    "    import os\n",
    "\n",
    "    zip_path = r\"../wikiextractor.zip\"\n",
    "    extract_to = r\"../\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    # Step 3: Delete the ZIP file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    # Step 4: Install Wikiextractor\n",
    "    !pip install -e ../wikiextractor-master\n",
    "else:\n",
    "    print(\"Wikiextractor already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990a714",
   "metadata": {},
   "source": [
    "Get wikipedia dump (takes like 2 hours to download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a29ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dump already downloaded\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(r\"../data/raw\", exist_ok=True)\n",
    "if not os.path.isfile(r\"../data/raw/enwiki-latest-pages-articles.xml.bz2\"):\n",
    "    !wget -P ../data/raw https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
    "else:\n",
    "    print(\"Wikipedia dump already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68e223",
   "metadata": {},
   "source": [
    "Install wikiextractor if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac50a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikiextractor is already installed.\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "try:\n",
    "    pkg_resources.get_distribution(\"wikiextractor\")\n",
    "    print(\"wikiextractor is already installed.\")\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"Installing wikiextractor...\")\n",
    "    !pip install -e ../wikiextractor-master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a9caa",
   "metadata": {},
   "source": [
    "Extract xml data from wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting page extraction from ../data/raw/enwiki-latest-pages-articles.xml.bz2.\n",
      "INFO: Using 11 extract processes.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(r\"../data/raw/extracted_wikidata\"):\n",
    "    !python -m wikiextractor.WikiExtractor \\\n",
    "        ../data/raw/enwiki-latest-pages-articles.xml.bz2 \\\n",
    "        -o ../data/raw/extracted_wikidata \\\n",
    "        --no-templates\n",
    "else:\n",
    "    print(\"Wikipedia XML extract already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0923b64",
   "metadata": {},
   "source": [
    "Create json data from wiki-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3df571c0340a9b0489157be8ccf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing XML files:   0%|          | 0/1 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.data_prep import traverse_directory\n",
    "\n",
    "input_dir = r'../data/raw/extracted_wikidata'\n",
    "output_dir = r'../data/processed/wikidata_json'\n",
    "\n",
    "traverse_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9736f8",
   "metadata": {},
   "source": [
    "Initialize spark session for metadata and triplets creation\n",
    "\n",
    "Create metadata from articles\n",
    "\n",
    "Create Training data triplets using Pyspark from json wikidata\n",
    "\n",
    "Had to install winutils for hadoop and pyspark to work on windows locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e93932",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"../data/processed/wikidata_json\"\n",
    "output_dir = r\"../data/processed/triplets/parts\"\n",
    "metadata_path = r\"../data/custom_model/article_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b85d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata already exists.\n",
      "Writing triplets to Spark part files: ../data/processed/triplets/parts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utils.spark_functions import create_article_metadata, create_paragraphs_df, create_triplets\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not os.path.isdir(output_dir) or os.path.isfile(metadata_path):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Capstone\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"20g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.local.dir\", \"../spark-temp\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"Loading JSON input\")\n",
    "    json_df = spark.read.option(\"multiLine\", True).json(f\"{input_dir}/**/*.json\")\n",
    "\n",
    "    # Create metadata file from articles\n",
    "    create_article_metadata(json_df, metadata_path)\n",
    "\n",
    "    # Logic for creating training data triplets from json\n",
    "    df = create_paragraphs_df(json_df)\n",
    "    triplets = create_triplets(df)\n",
    "    print(f\"Writing triplets to Spark part files: {output_dir}\")\n",
    "    triplets.write.mode(\"overwrite\").json(output_dir)\n",
    "\n",
    "    # Stop spark\n",
    "    spark.stop()\n",
    "else:\n",
    "    print(\"Triplets data has already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf9cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_prep import delete_files_in_dir_based_on_ext\n",
    "\n",
    "# Delete unneeded files produced by pyspark\n",
    "delete_files_in_dir_based_on_ext(output_dir, \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842330",
   "metadata": {},
   "source": [
    "Training embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4bd06",
   "metadata": {},
   "source": [
    "Embed and Index Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74c2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 22:27:43.546743: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 22:27:43.974103: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-13 22:27:43.974596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-13 22:27:43.976919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-13 22:27:44.189202: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ../data/custom_model/saved_vectorizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/custom_model/encoder_weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load vectorizer\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m load_triplet_dataset(input_dir, vectorizer, batch_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at ../data/custom_model/saved_vectorizer"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils.custom_embedder import *\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_dir = \"../data/processed/triplets/parts\"\n",
    "vectorizer_path = \"../data/custom_model/saved_vectorizer\"\n",
    "vocab_size = 30000\n",
    "max_len = 32\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "weights_path = \"../data/custom_model/encoder_weights.h5\"\n",
    "\n",
    "# Load vectorizer\n",
    "vectorizer = tf.keras.models.load_model(vectorizer_path)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = load_triplet_dataset(input_dir, vectorizer, batch_size)\n",
    "\n",
    "# Model setup\n",
    "encoder = CustomEncoder(vocab_size, max_len, embed_dim, num_heads, ff_dim, num_layers=2)\n",
    "trainer = TripletTrainer(encoder)\n",
    "trainer.compile(optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"loss\", patience=2),\n",
    "    ModelCheckpoint(\"../data/custom_model/best_encoder.keras\", monitor=\"loss\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "trainer.fit(train_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "\n",
    "# Save final encoder weights\n",
    "encoder.save_weights(weights_path)\n",
    "print(\"✅ Training complete and weights saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb2e0e",
   "metadata": {},
   "source": [
    "Create faiss index if not already existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dacdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 embedding files. Building FAISS index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c4379fad174c21a48bd65b7ba19333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing embedding files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "from utils.faiss_index import create_faiss_index_from_dir\n",
    "\n",
    "embedding_path = \"../data/custom_model/embeddings_output1\"\n",
    "faiss_path = \"../data/custom_model/faiss/faiss_index.index\"\n",
    "\n",
    "create_faiss_index_from_dir(embedding_path, faiss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdaaf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load saved vectorizer\n",
    "vectorizer = tf.keras.models.load_model(\"../data/custom_model/saved_vectorizer\")\n",
    "vectorizer = vectorizer.layers[0]\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = CustomEncoder(vocab_size=30000, max_len=32, embed_dim=128, num_heads=4, ff_dim=256)\n",
    "\n",
    "# Build the model by calling it on dummy data\n",
    "_ = encoder(tf.constant([[1] * 32]))  # shape: (1, max_len)\n",
    "\n",
    "# Now load the weights\n",
    "encoder.load_weights(\"../data/custom_model/encoder_weights.h5\")\n",
    "\n",
    "# Your query\n",
    "query = \"What is the function of DNA?\"\n",
    "\n",
    "query_seq = vectorizer(tf.constant([query]))\n",
    "query_embedding = encoder(query_seq).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e839dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.faiss_index import query_faiss\n",
    "\n",
    "indices = query_faiss(faiss_path, query_embedding, 10)\n",
    "\n",
    "# Load article metadata\n",
    "metadata_path = \"../data/custom_model/article_metadata.json\"\n",
    "with open(metadata_path, encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Retrieve top-k articles\n",
    "results = [metadata[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01108fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61056661e084ddea32f7e64737a85a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading embeddings:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from glob import glob\n",
    "\n",
    "# Gather all article embeddings from .npy files\n",
    "all_embeddings = []\n",
    "embedding_files = sorted(glob(os.path.join(embedding_path, '*.npy')))\n",
    "for file_path in tqdm(embedding_files, desc=\"Loading embeddings\"):\n",
    "    all_embeddings.append(np.load(file_path))\n",
    "\n",
    "# Stack into one large matrix (shape: [N, dim])\n",
    "article_embeddings = np.vstack(all_embeddings)  # Now shape: (N_total, dim)\n",
    "\n",
    "# Create combined metadata\n",
    "top_articles = [metadata[i] | {\"vec\": article_embeddings[i]} for i in indices[0]]\n",
    "\n",
    "# Rerank using cosine similarity\n",
    "query_vec = query_embedding[0].reshape(1, -1)\n",
    "top_articles.sort(\n",
    "    key=lambda x: cosine_similarity(query_vec, x[\"vec\"].reshape(1, -1))[0][0],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Whitson, Texas',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74340115'},\n",
      " {'title': 'My Kink Is Karma',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74920372'},\n",
      " {'title': 'Marcial Moreno-Mañas',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74635281'},\n",
      " {'title': 'Iran at the 2022 Asian Games',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74864425'},\n",
      " {'title': 'Sir Charles Saxton, 2nd Baronet',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74896144'},\n",
      " {'title': \"List of Girls' Crystal comic stories\",\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74899201'},\n",
      " {'title': 'Not My Neighbour',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74886103'},\n",
      " {'title': 'Alaska Airlines Flight 2059',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=75127810'},\n",
      " {'title': 'Ting Ting Chaoro',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74877608'},\n",
      " {'title': 'Detdet Pepito',\n",
      "  'url': 'https://en.wikipedia.org/wiki?curid=74590708'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "final_results = [\n",
    "    {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": article.get(\"url\", \"N/A\")\n",
    "    }\n",
    "    # for article in top_articles[:5]\n",
    "    for article in top_articles\n",
    "]\n",
    "pprint.pprint(final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
