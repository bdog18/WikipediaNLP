{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ce7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add root directory (one level up from notebooks/)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc0ea6",
   "metadata": {},
   "source": [
    "download and install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "100 49509  100 49509    0     0  92640      0 --:--:-- --:--:-- --:--:-- 92640\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download the ZIP file\n",
    "!curl -L -o ../wikiextractor.zip https://github.com/qfcy/wikiextractor/archive/refs/heads/master.zip\n",
    "\n",
    "# Step 2: Extract it\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"../wikiextractor.zip\"\n",
    "extract_to = \"../\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "# Step 3: Delete the ZIP file\n",
    "os.remove(zip_path)\n",
    "\n",
    "# Step 4: Install Wikiextractor\n",
    "!pip install -e ../wikiextractor-master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990a714",
   "metadata": {},
   "source": [
    "Get wikipedia dump (takes like 2 hours to download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "!wget -P ../data/raw https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a9caa",
   "metadata": {},
   "source": [
    "Extract xml data from wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m wikiextractor.WikiExtractor \\\n",
    "    ../data/raw/enwiki-latest-pages-articles.xml.bz2 \\\n",
    "    -o ../data/raw/extracted_wikidata \\\n",
    "    --no-templates \\\n",
    "    --processes 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0923b64",
   "metadata": {},
   "source": [
    "Create json data from wiki-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea74af9f39549ca892cae0aa2f3aee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing XML files:   0%|          | 0/19038 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.data_prep import traverse_directory\n",
    "\n",
    "input_dir = r'../data/raw/extracted_wikidata'\n",
    "output_dir = r'../data/processed/wikidata_json'\n",
    "\n",
    "traverse_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9736f8",
   "metadata": {},
   "source": [
    "Create Training data triplets using Pyspark from json wikidata\n",
    "\n",
    "Had to install winutils for hadoop and pyspark to work on windows locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e93932",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"../data/processed/wikidata_json\"\n",
    "output_dir = r\"../data/processed/triplets/combined\"\n",
    "# output_dir = r\"../data/processed/triplets/parts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON input\n",
      "Writing triplets to Spark part files: ../data/processed/triplets/parts\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utils.generate_triplets import create_paragraphs_df, create_triplets\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TripletCreator\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"24g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.local.dir\", \"../spark-temp\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = create_paragraphs_df(spark, input_dir)\n",
    "triplets = create_triplets(df)\n",
    "print(f\"Writing triplets to Spark part files: {output_dir}\")\n",
    "triplets.coalesce(1).write.mode(\"overwrite\").json(output_dir) # for single json file\n",
    "# triplets.write.mode(\"overwrite\").json(output_dir) # for multiple json files\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b94cb",
   "metadata": {},
   "source": [
    "Delete unneeded files produced by pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ../data/processed/triplets/combined\\.part-00000-4d87b85e-2b0d-4eda-b097-a980c3c35e5d-c000.json.crc\n",
      "Deleted: ../data/processed/triplets/combined\\._SUCCESS.crc\n",
      "Preserved: ../data/processed/triplets/combined\\part-00000-4d87b85e-2b0d-4eda-b097-a980c3c35e5d-c000.json\n",
      "Deleted: ../data/processed/triplets/combined\\_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "from utils.data_prep import delete_files_in_dir_based_on_ext\n",
    "delete_files_in_dir_based_on_ext(output_dir, \".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c842330",
   "metadata": {},
   "source": [
    "Training embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a201bc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd82b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d4bd06",
   "metadata": {},
   "source": [
    "Embed and Index Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c2a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aecb2e0e",
   "metadata": {},
   "source": [
    "Create faiss index if not already existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dacdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.faiss_index import create_faiss_index, query_faiss\n",
    "\n",
    "faiss_path = \"../data/embeddings/faiss_index.index\"\n",
    "embedding_path = \"../data/embeddings/article_embeddings.npy\"\n",
    "\n",
    "create_faiss_index(faiss_path, embedding_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
